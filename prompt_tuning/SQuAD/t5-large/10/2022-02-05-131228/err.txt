Reusing dataset mrqa (/datasets/home/37/137/ziw029/T5_SQuAD_Prompt_Tuning/data/mrqa/mrqa/plain_text/1.1.0/1f2cf5ac32b43f864e6f91d384057a16b69b7d13ba9bcaa200ac277c90938d19)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 94.70it/s]
Traceback (most recent call last):
  File "/datasets/home/home-01/37/137/ziw029/T5_SQuAD_Prompt_Tuning/run_prompt_tuning.py", line 39, in <module>
    main(args)
  File "/datasets/home/home-01/37/137/ziw029/T5_SQuAD_Prompt_Tuning/run_prompt_tuning.py", line 36, in main
    engine_prompt_tuning.run(args)
  File "/datasets/home/home-01/37/137/ziw029/T5_SQuAD_Prompt_Tuning/engine_prompt_tuning.py", line 170, in run
    compute_metrics(model_data_wrapper)
  File "/datasets/home/home-01/37/137/ziw029/T5_SQuAD_Prompt_Tuning/engine_prompt_tuning.py", line 138, in compute_metrics
    test_set_pred = generate_predictions(model, tokenizer, test_set)
  File "/datasets/home/home-01/37/137/ziw029/T5_SQuAD_Prompt_Tuning/engine_prompt_tuning.py", line 122, in generate_predictions
    idx = model(input_ids, decoder_input_ids=decoder_input_ids, return_dict=True).logits.argmax(-1)[0][-1]
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/datasets/home/home-01/37/137/ziw029/T5_SQuAD_Prompt_Tuning/model_prompt_tuning.py", line 159, in forward
    return super().forward(
  File "/home/ziw029/.local/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 1571, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ziw029/.local/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 1003, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ziw029/.local/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 639, in forward
    self_attention_outputs = self.layer[0](
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ziw029/.local/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 546, in forward
    attention_output = self.SelfAttention(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ziw029/.local/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 509, in forward
    attn_weights = nn.functional.dropout(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py", line 1168, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.92 GiB total capacity; 9.91 GiB already allocated; 11.38 MiB free; 9.96 GiB reserved in total by PyTorch)
